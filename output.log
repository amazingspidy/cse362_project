nohup: ignoring input
Epoch [1/1], Batch [1], Loss: 2.0121
Epoch [1/1], Batch [11], Loss: 1.1591
Epoch [1/1], Batch [21], Loss: 0.9420
Epoch [1/1], Batch [31], Loss: 0.7843
Epoch [1/1], Batch [41], Loss: 0.6548
Epoch [1/1], Batch [51], Loss: 0.6114
Epoch [1/1], Batch [61], Loss: 0.6296
Epoch [1/1], Batch [71], Loss: 0.6653
Epoch [1/1], Batch [81], Loss: 0.4757
Epoch [1/1], Batch [91], Loss: 0.4799
Epoch [1/1], Batch [101], Loss: 0.5966
Epoch [1/1], Batch [111], Loss: 0.5822
Epoch [1/1], Batch [121], Loss: 0.5417
Epoch [1/1], Batch [131], Loss: 0.5142
Epoch [1/1], Batch [141], Loss: 0.6530
Epoch [1/1], Batch [151], Loss: 0.5061
Epoch [1/1], Batch [161], Loss: 0.4931
Epoch [1/1], Batch [171], Loss: 0.4551
Epoch [1/1], Batch [181], Loss: 0.4658
Epoch [1/1], Batch [191], Loss: 0.4576
Epoch [1/1], Batch [201], Loss: 0.4773
Epoch [1/1], Batch [211], Loss: 0.5247
Epoch [1/1], Batch [221], Loss: 0.5053
Epoch [1/1], Batch [231], Loss: 0.5020
Epoch [1/1], Batch [241], Loss: 0.4534
Epoch [1/1], Batch [251], Loss: 0.4460
Epoch [1/1], Batch [261], Loss: 0.4933
Epoch [1/1], Batch [271], Loss: 0.4799
Epoch [1/1], Batch [281], Loss: 0.4265
Epoch [1/1], Batch [291], Loss: 0.4907
Epoch [1/1], Batch [301], Loss: 0.4710
Epoch [1/1], Batch [311], Loss: 0.4758
Epoch [1/1], Batch [321], Loss: 0.4780
Epoch [1/1], Batch [331], Loss: 0.4224
Epoch [1/1], Batch [341], Loss: 0.5235
Epoch [1/1], Batch [351], Loss: 0.3872
Epoch [1/1], Batch [361], Loss: 0.3931
Epoch [1/1], Batch [371], Loss: 0.5080
Epoch [1/1], Batch [381], Loss: 0.4260
Epoch [1/1], Batch [391], Loss: 0.4534
Epoch [1/1], Batch [401], Loss: 0.4877
Epoch [1/1], Batch [411], Loss: 0.4828
Epoch [1/1], Batch [421], Loss: 0.4089
Epoch [1/1], Batch [431], Loss: 0.4780
Epoch [1/1], Batch [441], Loss: 0.4338
Epoch [1/1], Batch [451], Loss: 0.4835
Epoch [1/1], Batch [461], Loss: 0.4988
Epoch [1/1], Batch [471], Loss: 0.4208
Epoch [1/1], Batch [481], Loss: 0.4828
Epoch [1/1], Batch [491], Loss: 0.4084
Epoch [1/1], Batch [501], Loss: 0.4140
Epoch [1/1], Batch [511], Loss: 0.4759
Epoch [1/1], Batch [521], Loss: 0.3329
Epoch [1/1], Batch [531], Loss: 0.6079
Epoch [1/1], Batch [541], Loss: 0.4494
Epoch [1/1], Batch [551], Loss: 0.4412
Epoch [1/1], Batch [561], Loss: 0.5428
Epoch [1/1], Batch [571], Loss: 0.4233
Epoch [1/1], Batch [581], Loss: 0.3939
Epoch [1/1], Batch [591], Loss: 0.4667
Epoch [1/1], Batch [601], Loss: 0.5417
Epoch [1/1], Batch [611], Loss: 0.5440
Epoch [1/1], Batch [621], Loss: 0.4930
Epoch [1/1], Batch [631], Loss: 0.4064
Epoch [1/1], Batch [641], Loss: 0.4715
Epoch [1/1], Batch [651], Loss: 0.4411
Epoch [1/1], Batch [661], Loss: 0.4496
Epoch [1/1], Batch [671], Loss: 0.3834
Epoch [1/1], Batch [681], Loss: 0.4586
Epoch [1/1], Batch [691], Loss: 0.3438
Epoch [1/1], Batch [701], Loss: 0.4218
Epoch [1/1], Batch [711], Loss: 0.4488
Epoch [1/1], Batch [721], Loss: 0.5032
Epoch [1/1], Batch [731], Loss: 0.4575
Epoch [1/1], Batch [741], Loss: 0.5150
Epoch [1/1], Batch [751], Loss: 0.3227
Epoch [1/1], Batch [761], Loss: 0.3960
Epoch [1/1], Batch [771], Loss: 0.4269
Epoch [1/1], Batch [781], Loss: 0.4779
Epoch [1/1], Batch [791], Loss: 0.4892
Epoch [1/1], Batch [801], Loss: 0.4749
Epoch [1/1], Batch [811], Loss: 0.3761
Epoch [1/1], Batch [821], Loss: 0.4693
Epoch [1/1], Batch [831], Loss: 0.5495
Epoch [1/1], Batch [841], Loss: 0.4410
Epoch [1/1], Batch [851], Loss: 0.5213
Epoch [1/1], Batch [861], Loss: 0.4994
Epoch [1/1], Batch [871], Loss: 0.4394
Epoch [1/1], Batch [881], Loss: 0.4724
Epoch [1/1], Batch [891], Loss: 0.4273
Epoch [1/1], Batch [901], Loss: 0.5347
Epoch [1/1], Batch [911], Loss: 0.4705
Model saved to multifusion_model.pth
